{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f333bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Created variables:\n",
      "inp_dir = /scratch/htc/fsafarov/structures/8ef5_july_2025/8ef5/\n",
      "dcd_dir = /scratch/htc/fsafarov/mOR_dcd_files/npat/\n",
      "out_dir = /scratch/htc/fsafarov/ISOKANN_PINN/output/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch as pt\n",
    "from torch.nn.functional import normalize\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import sys\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.func import hessian, vmap\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = pt.device(\"cuda\" if pt.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from src.useful_functions import *\n",
    "from src.PWDs_module import generate_PWDistances_torch\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# Read directory paths\n",
    "read_dirs_paths('dir_paths_.txt', globals())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b06b79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pt.nn.Module):\n",
    "\n",
    "    def __init__(self, Nodes, enforce_positive=0, act_fun='sigmoid', LeakyReLU_par=0.01):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.input_size    = Nodes[0]\n",
    "        self.output_size   = Nodes[-1]\n",
    "        self.Nhiddenlayers = len(Nodes)-2\n",
    "        self.Nodes         = Nodes\n",
    "\n",
    "        dims_in = Nodes[:-1]\n",
    "        dims_out = Nodes[1:]\n",
    "\n",
    "        if act_fun == 'sigmoid':\n",
    "            self.activation  = pt.nn.Sigmoid()  # #\n",
    "        elif act_fun == 'relu':\n",
    "            self.activation  = pt.nn.ReLU()\n",
    "        elif act_fun == 'leakyrelu': \n",
    "            self.activation  = pt.nn.LeakyReLU(LeakyReLU_par)\n",
    "        elif act_fun == 'gelu': \n",
    "            self.activation  = pt.nn.GELU()\n",
    "\n",
    "            \n",
    "        layers = []\n",
    "\n",
    "        for i, (dim_in, dim_out) in enumerate(zip(dims_in, dims_out)):\n",
    "            layers.append(torch.nn.Linear(dim_in, dim_out))\n",
    "\n",
    "            if i < self.Nhiddenlayers:\n",
    "                layers.append(self.activation)\n",
    "\n",
    "        self._layers = torch.nn.Sequential(*layers)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            MLP forward pass\n",
    "        \"\"\"\n",
    "        return self._layers(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6328/3026513343.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = pt.tensor(X, dtype=pt.float32, device=device)\n",
      "  0%|          | 0/10000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1016, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     94\u001b[39m net = MLP([dim, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m1\u001b[39m], act_fun=\u001b[33m'\u001b[39m\u001b[33mgelu\u001b[39m\u001b[33m'\u001b[39m).to(device)\n\u001b[32m     95\u001b[39m forces_fn = \u001b[38;5;28;01mlambda\u001b[39;00m x: -potential_grad_fn(pt.tensor(x)) / gamma  \u001b[38;5;66;03m# User impl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m train_losses, val_losses = \u001b[43mtrainNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforces_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforces_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrainNN\u001b[39m\u001b[34m(model, lr, wd, Nepochs, batch_size, patience, X, Y, forces_fn, kB, T, gamma, c1, c2, split)\u001b[39m\n\u001b[32m     48\u001b[39m idx = perm[i:i+batch_size]\n\u001b[32m     49\u001b[39m batch_x = X_train[idx]\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m L_chi = \u001b[43mgenerator_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforces_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m chi = model(batch_x)\n\u001b[32m     53\u001b[39m residual = L_chi + c1 * chi.squeeze() - c2 * (\u001b[32m1\u001b[39m - chi.squeeze())  \u001b[38;5;66;03m# (B,)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mgenerator_action\u001b[39m\u001b[34m(model, x, forces_fn, D)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"ℒχ = b · ∇χ + D Δχ\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m grad_chi = nabla_chi(model, x)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m lap_chi = \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlaplacian_operator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# vmap over batch\u001b[39;00m\n\u001b[32m     21\u001b[39m b = forces_fn(x)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (b * grad_chi).sum(-\u001b[32m1\u001b[39m) + D * lap_chi\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/apis.py:202\u001b[39m, in \u001b[36mvmap.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/vmap.py:334\u001b[39m, in \u001b[36mvmap_impl\u001b[39m\u001b[34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[32m    324\u001b[39m         func,\n\u001b[32m    325\u001b[39m         flat_in_dims,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m         **kwargs,\n\u001b[32m    331\u001b[39m     )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/vmap.py:484\u001b[39m, in \u001b[36m_flat_vmap\u001b[39m\u001b[34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[32m    481\u001b[39m     batched_inputs = _create_batched_inputs(\n\u001b[32m    482\u001b[39m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[32m    483\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m     batched_outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mlaplacian_operator\u001b[39m\u001b[34m(model, x)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlaplacian_operator\u001b[39m(model, x):\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Δχ = trace(Hessian χ), scalar χ: x(B,D) → scalar\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     H = \u001b[43mhessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, D, D)\u001b[39;00m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pt.trace(H, dim1=-\u001b[32m2\u001b[39m, dim2=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:1258\u001b[39m, in \u001b[36mjacfwd.<locals>.wrapper_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m   1256\u001b[39m flat_primals, primals_spec = tree_flatten(primals)\n\u001b[32m   1257\u001b[39m flat_primals_numels = \u001b[38;5;28mtuple\u001b[39m(p.numel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m flat_primals)\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m flat_basis = \u001b[43m_construct_standard_basis_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_primals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_primals_numels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1259\u001b[39m basis = tree_unflatten(flat_basis, primals_spec)\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpush_jvp\u001b[39m(basis):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:825\u001b[39m, in \u001b[36m_construct_standard_basis_for\u001b[39m\u001b[34m(tensors, tensor_numels)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_construct_standard_basis_for\u001b[39m(tensors, tensor_numels):\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_chunked_standard_basis_for_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:810\u001b[39m, in \u001b[36m_chunked_standard_basis_for_\u001b[39m\u001b[34m(tensors, tensor_numels, chunk_size)\u001b[39m\n\u001b[32m    804\u001b[39m diag_start_indices = (\n\u001b[32m    805\u001b[39m     \u001b[32m0\u001b[39m,\n\u001b[32m    806\u001b[39m     *torch.tensor(tensor_numels).cumsum(dim=\u001b[32m0\u001b[39m)[:-\u001b[32m1\u001b[39m].neg().unbind(),\n\u001b[32m    807\u001b[39m )\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk_idx, total_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk_numels):\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m     chunks = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_numel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk, diag_start_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, diag_start_indices):\n\u001b[32m    816\u001b[39m         chunk.diagonal(diag_start_idx + chunk_idx * chunk_size).fill_(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/htc/fsafarov/openmm_ff/lib/python3.12/site-packages/torch/_functorch/eager_transforms.py:811\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    804\u001b[39m diag_start_indices = (\n\u001b[32m    805\u001b[39m     \u001b[32m0\u001b[39m,\n\u001b[32m    806\u001b[39m     *torch.tensor(tensor_numels).cumsum(dim=\u001b[32m0\u001b[39m)[:-\u001b[32m1\u001b[39m].neg().unbind(),\n\u001b[32m    807\u001b[39m )\n\u001b[32m    809\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk_idx, total_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunk_numels):\n\u001b[32m    810\u001b[39m     chunks = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m811\u001b[39m         \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_numel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_numel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m tensor, tensor_numel \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, tensor_numels)\n\u001b[32m    813\u001b[39m     )\n\u001b[32m    815\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk, diag_start_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(chunks, diag_start_indices):\n\u001b[32m    816\u001b[39m         chunk.diagonal(diag_start_idx + chunk_idx * chunk_size).fill_(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1016, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def nabla_chi(model, x):\n",
    "    \"\"\"Gradient ∇χ(x), assuming scalar χ: x(B,D) → (B,D)\"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    chi = model(x)\n",
    "    grad_chi = pt.autograd.grad(chi.sum(), x, create_graph=True, retain_graph=True)[0]\n",
    "    return grad_chi  # (B, D)\n",
    "\n",
    "def laplacian_operator(model, x):\n",
    "    \"\"\"Δχ = trace(Hessian χ), scalar χ: x(B,D) → scalar\"\"\"\n",
    "    H = hessian(model)(x)  # (B, D, D)\n",
    "    return pt.trace(H, dim1=-2, dim2=-1)  # (B,)\n",
    "\n",
    "def generator_action(model, x, forces_fn, D):  # forces_fn(x) → b(x) (B,D)\n",
    "   \n",
    "    grad_chi = nabla_chi(model, x)\n",
    "    lap_chi = vmap(laplacian_operator, in_dims=(None, 0))(model, x)  # vmap over batch\n",
    "    b = forces_fn(x)\n",
    "    return (b * grad_chi).sum(-1) + D * lap_chi  # (B,)\n",
    "\n",
    "def trainNN(model, lr=1e-3, wd=1e-5, Nepochs=10000, batch_size=1024, patience=50,\n",
    "            X=None, Y=None, forces_fn=None, kB=1.0, T=300, gamma=1.0, c1=0.01, c2=0.01,\n",
    "            split=0.2):\n",
    "    model.to(device)\n",
    "    D = kB * T / gamma  # Diffusion const\n",
    "\n",
    "    if X is not None:\n",
    "        X = pt.tensor(X, dtype=pt.float32, device=device)\n",
    "        X_train, X_val, _, _ = train_test_split(X, Y, test_size=split)\n",
    "\n",
    "    optimizer = pt.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    mse_loss = pt.nn.MSELoss()\n",
    "\n",
    "    best_loss, patience_counter = float('inf'), 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in tqdm(range(Nepochs)):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        perm = pt.randperm(X_train.size(0), device=device)\n",
    "\n",
    "        for i in range(0, X_train.size(0), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            idx = perm[i:i+batch_size]\n",
    "            batch_x = X_train[idx]\n",
    "\n",
    "            L_chi = generator_action(model, batch_x, forces_fn, D)\n",
    "            chi = model(batch_x)\n",
    "            residual = L_chi + c1 * chi.squeeze() - c2 * (1 - chi.squeeze())  # (B,)\n",
    "            pde_loss = mse_loss(residual, pt.zeros_like(residual))\n",
    "            reg_loss = mse_loss(chi.squeeze(), chi.squeeze() * 0) + mse_loss(1-chi.squeeze(), (1-chi.squeeze()) * 0)  # Soft [0,1]\n",
    "            loss = pde_loss + 0.01 * reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            pt.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Val\n",
    "        model.eval()\n",
    "        with pt.no_grad():\n",
    "            L_chi_val = generator_action(model, X_val, forces_fn, D)\n",
    "            chi_val = model(X_val)\n",
    "            res_val = L_chi_val + c1 * chi_val.squeeze() - c2 * (1 - chi_val.squeeze())\n",
    "            val_loss = mse_loss(res_val, pt.zeros_like(res_val)).item()\n",
    "\n",
    "        train_losses.append(train_loss / (X_train.size(0)//batch_size))\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            pt.save(model.state_dict(), 'best_chi.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter > patience:\n",
    "                break\n",
    "\n",
    "    logging.info(f\"Best val loss: {best_loss}\")\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "D0 = pt.load(out_dir + 'PWDistances_0.pt', map_location=device)\n",
    "dim = D0.shape[-1]\n",
    "# Usage ex.\n",
    "net = MLP([dim, 256, 256, 256, 1], act_fun='gelu').to(device)\n",
    "forces_fn = lambda x: -potential_grad_fn(pt.tensor(x)) / gamma  # User impl\n",
    "train_losses, val_losses = trainNN(net, X=D0, Y=D0, forces_fn=forces_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1b6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenMM (CUDA2)",
   "language": "python",
   "name": "openmm_ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
